{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    root_folder = \"/Users/mohithsai/Artificial Intelligance/Llama/dataset\"\n",
    "    loader = DirectoryLoader(\n",
    "        path=root_folder,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        loader_kwargs={\"extract_images\": True},\n",
    "        recursive=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    db.save_local(\"faiss_index\")\n",
    "\n",
    "    return db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "\n",
    "    return FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True).as_retriever(\n",
    "        search_type='mmr', search_kwargs={\"k\": 3}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_qa_chain(retriever):\n",
    "    system_prompt = (\n",
    "        \"You are a highly knowledgeable and empathetic medical assistant. \"\n",
    "        \"Your primary goal is to provide information and guidance on medical conditions, symptoms, and treatments. \"\n",
    "        \"Respond with clear, concise, and fact-based medical advice, emphasizing the importance of consulting a healthcare professional for personalized care.\\n\\n\"\n",
    "        \"Guidelines:\\n\"\n",
    "        \"- Analyze the userâ€™s symptoms or condition based on the provided context and any relevant information you have.\\n\"\n",
    "        \"- Suggest potential treatment options, lifestyle modifications, or preventive measures. Provide this information as general advice that might be helpful, rather than a personalized treatment plan.\\n\"\n",
    "        \"- If the question pertains to serious or complex conditions, encourage the user to seek immediate assistance from a healthcare provider.\\n\"\n",
    "        \"- Keep responses understandable by a general audience and avoid complex jargon unless it is explained.\\n\"\n",
    "        \"- Be clear when certain symptoms or conditions fall outside of your informational scope and advise users to consult a medical professional for further assessment.\\n\\n\"\n",
    "        \"Sample Response Structure:\\n\"\n",
    "        \"1. **Identification of Condition:** Start by acknowledging the condition or symptoms described and provide a brief overview.\\n\"\n",
    "        \"2. **General Treatment Options:** Outline commonly recommended treatments, such as medications, therapies, or over-the-counter options, if applicable.\\n\"\n",
    "        \"3. **Lifestyle Recommendations:** Include helpful lifestyle tips that may support symptom management (e.g., dietary changes, sleep habits, physical activity).\\n\"\n",
    "        \"4. **When to Seek Help:** Advise on signs that may require professional intervention and encourage regular check-ins with a healthcare provider.\\n\"\n",
    "        \"5. **Disclaimer:** Remind users that this information is not a substitute for professional medical advice and is for informational purposes only.\\n\\n\"\n",
    "        \"Context: {context}\"\n",
    "    )\n",
    "\n",
    "    model = OllamaLLM(\n",
    "        model='llama3.1',\n",
    "        temperature=0.7,  \n",
    "        max_tokens=512,   \n",
    "        top_p=0.85         \n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    stuff_chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever,\n",
    "        stuff_chain\n",
    "    )\n",
    "\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    retriever = load_faiss_index() if os.path.exists(\"faiss_index\") else create_retriever(load_documents())\n",
    "\n",
    "    chain = build_retrieval_qa_chain(retriever)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        user_question = input(\"Type your medical question here (or type 'exit' to quit): \")\n",
    "\n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Exiting the chatbot. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if user_question:\n",
    "            try:\n",
    "                response = chain.invoke({\"input\": user_question})\n",
    "\n",
    "                if 'answer' in response:\n",
    "                    print(\"Response:\", response['answer'])\n",
    "                \n",
    "                if 'context' in response:\n",
    "                    print(\"\\nSource Documents:\")\n",
    "                    for doc in response['context']:\n",
    "                        print(f\" - Source: {doc.metadata['source']}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"An error occurred while processing your question:\", str(e))\n",
    "        else:\n",
    "            print(\"Please enter a valid question.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:12:54.776 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    st.title(\"Health Chatbot\")\n",
    "    st.write(\"Ask your medical questions below:\")\n",
    "\n",
    "    if os.path.exists(\"faiss_index\"):\n",
    "        retriever = load_faiss_index()\n",
    "    else:\n",
    "        retriever = create_retriever(load_documents())\n",
    "\n",
    "    chain = build_retrieval_qa_chain(retriever)\n",
    "\n",
    "    user_question = st.text_input(\"Type your medical question here:\")\n",
    "\n",
    "    if user_question:\n",
    "        try:\n",
    "            response = chain.invoke({\"input\": user_question})\n",
    "\n",
    "            if 'answer' in response:\n",
    "                st.write(\"Response:\", response['answer'])\n",
    "            \n",
    "            if 'context' in response:\n",
    "                st.write(\"Source Documents:\")\n",
    "                for doc in response['context']:\n",
    "                    st.write(f\" - Source: {doc.metadata['source']}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
